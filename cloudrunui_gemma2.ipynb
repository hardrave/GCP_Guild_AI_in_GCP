{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardrave/GCP_Guild_AI_in_GCP/blob/main/cloudrunui_gemma2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GCP Guild Association: Deploying Ollama as a Sidecar with Cloud Run and Open WebUI\n",
        "\n",
        "This collaborative notebook was created for the GCP Guild AOssociation to demonstrate how to deploy Ollama as a sidecar container with Cloud Run, using Open WebUI as the frontend ingress container. The notebook provides a step-by-step guide on setting up the Cloud Run environment, configuring resources, containerizing both Ollama and Open WebUI, and deploying them as a multi-container Cloud Run service."
      ],
      "metadata": {
        "id": "x5mKxVeNks6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate with Google Cloud\n",
        "\n",
        "This cell authenticates your Google Cloud account using the `gcloud` command-line tool. It updates the application default credentials (ADC) and runs in quiet mode."
      ],
      "metadata": {
        "id": "q9dvbLqKgokP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lWkochVBM6g"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login --update-adc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Vertex AI with Project and Location\n",
        "\n",
        "This cell initializes the Vertex AI SDK with your Google Cloud project and location settings.\n",
        "\n",
        "1. **Import necessary libraries:** Imports the `os` module for environment variables and the `vertexai` library for Vertex AI interactions.\n",
        "2. **Project ID:** Sets the `PROJECT_ID` variable. If you provide a value in the Colab interface, it uses that. Otherwise, it automatically retrieves the project ID from the `GOOGLE_CLOUD_PROJECT` environment variable.\n",
        "3. **Location:** Sets the `LOCATION` variable. It defaults to \"us-central1\" if the `GOOGLE_CLOUD_REGION` environment variable is not set.\n",
        "4. **Initialize Vertex AI:** Initializes the Vertex AI SDK with the specified project and location, enabling you to use Vertex AI services in your code."
      ],
      "metadata": {
        "id": "yZFSi6njhkEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfTWMeZTBM6h"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\", isTemplate: true}\n",
        "if PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Artifact Repository Name\n",
        "\n",
        "This cell defines a variable to store the name of your Artifact Registry repository.\n",
        "\n",
        "*   **AR_REPOSITORY_NAME:** This variable is assigned the value \"ollama-sidecar-codelab\", which will be used as the name of the repository for storing your Docker images."
      ],
      "metadata": {
        "id": "YU30GMMMhmn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nb9rfoyBM6i"
      },
      "outputs": [],
      "source": [
        "AR_REPOSITORY_NAME = \"ollama-sidecar-codelab\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Artifact Registry Repository\n",
        "\n",
        "This cell creates a new Artifact Registry repository to store your Docker images.\n",
        "\n",
        "*   **gcloud artifacts repositories create:** This command uses the `gcloud` command-line tool to create a new Artifact Registry repository.\n",
        "*   **AR_REPOSITORY_NAME:** This variable is replaced with the name you defined earlier (\"ollama-sidecar-codelab\").\n",
        "*   **--repository-format=docker:** Specifies that the repository will store Docker images.\n",
        "*   **--location=LOCATION:** Uses the location you specified earlier (e.g., \"us-central1\").\n",
        "*   **--project=$PROJECT_ID:** Uses the project ID you specified or retrieved from the environment variable."
      ],
      "metadata": {
        "id": "l-PzQAwCh5AU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKvNVcRJBM6i"
      },
      "outputs": [],
      "source": [
        "!gcloud artifacts repositories create $AR_REPOSITORY_NAME \\\n",
        "      --repository-format=docker \\\n",
        "      --location=$LOCATION \\\n",
        "      --project=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model Name\n",
        "\n",
        "This cell defines a variable to store the name of your machine learning model.\n",
        "\n",
        "*   **MODEL_NAME:** This variable is assigned the value \"gemma2:2b\", which represents the name and version of the model you'll be using."
      ],
      "metadata": {
        "id": "AgNtOSeDiJBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAcsdAV9BM6j"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gemma2:2b\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dockerfile for Ollama Model Serving\n",
        "\n",
        "This cell creates a Dockerfile that defines the environment for serving your Ollama model.\n",
        "\n",
        "1. **Define Dockerfile content:** It creates a multiline string (`dockerfile_content`) containing the instructions for building the Docker image.\n",
        "    * **Base Image:** It starts with the `ollama/ollama` base image.\n",
        "    * **Environment Variables:** It sets environment variables for Ollama, such as the host, model storage, logging, and keep-alive settings.\n",
        "    * **Model Download:** It downloads the specified model weights using `ollama pull`.\n",
        "    * **Entrypoint:** It defines the command to run when the container starts, which is `ollama serve` to start the Ollama server.\n",
        "2. **Write Dockerfile:** It writes the content of `dockerfile_content` to a file named \"Dockerfile\" in your Colab environment."
      ],
      "metadata": {
        "id": "oTjgpN0oiPfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAlpMw5dBM6j"
      },
      "outputs": [],
      "source": [
        "dockerfile_content = f\"\"\"\n",
        "FROM ollama/ollama\n",
        "\n",
        "# Listen on all interfaces, port 11434\n",
        "ENV OLLAMA_HOST 0.0.0.0:11434\n",
        "\n",
        "# Store model weight files in /models\n",
        "ENV OLLAMA_MODELS /models\n",
        "\n",
        "# Reduce logging verbosity\n",
        "ENV OLLAMA_DEBUG false\n",
        "\n",
        "# Never unload model weights from the GPU\n",
        "ENV OLLAMA_KEEP_ALIVE -1\n",
        "\n",
        "# Store the model weights in the container image\n",
        "ENV MODEL gemma2:2b\n",
        "RUN ollama serve & sleep 5 && ollama pull $MODEL\n",
        "\n",
        "# Start Ollama\n",
        "ENTRYPOINT [\"ollama\", \"serve\"]\n",
        "\"\"\"\n",
        "\n",
        "# Write the Dockerfile\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Push Docker Image\n",
        "\n",
        "This cell builds a Docker image using Cloud Build and pushes it to Artifact Registry.\n",
        "\n",
        "*   **gcloud builds submit:** This command initiates a Cloud Build job to build the Docker image based on the Dockerfile you created.\n",
        "*   **--project PROJECT_ID:** Specifies the Google Cloud project ID for the build.\n",
        "*   **--tag us-central1-docker.pkg.dev/$PROJECT_ID/ollama-sidecar-codelab-repo/ollama-gemma-2b:**  Tags the built image with a unique identifier, including the Artifact Registry repository path. This tag is used for pushing the image to Artifact Registry.\n",
        "*   **--machine-type e2-highcpu-32:** Specifies the machine type to use for the build process, selecting a high-CPU machine for faster builds."
      ],
      "metadata": {
        "id": "MkI2nTmiiYga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwCiRTFTBM6j"
      },
      "outputs": [],
      "source": [
        "!gcloud builds submit --project $PROJECT_ID \\\n",
        "   --tag us-central1-docker.pkg.dev/$PROJECT_ID/ollama-sidecar-codelab-repo/ollama-gemma-2b \\\n",
        "   --machine-type e2-highcpu-32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pull Open Web UI Image\n",
        "\n",
        "This cell pulls the Open Web UI Docker image from the GitHub Container Registry.\n",
        "\n",
        "*   **docker pull:** This command is used to download a Docker image from a registry.\n",
        "*   **ghcr.io/open-webui/open-webui:main:** Specifies the image to pull, which is the Open Web UI image from the GitHub Container Registry, using the `main` tag."
      ],
      "metadata": {
        "id": "MyfBQAB1ipc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECuwgiu2BM6j"
      },
      "outputs": [],
      "source": [
        "!docker pull ghcr.io/open-webui/open-webui:main\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install gcr.io/go-containerregistry/crane:latest\n",
        "This cell installs the `crane` tool, which is used for interacting with container registries.\n",
        "\n",
        "* `!gcrane`: This command downloads and installs the `crane` tool from the Google Container Registry."
      ],
      "metadata": {
        "id": "RBskp1k5i3U5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noIFMSaTBM6k"
      },
      "outputs": [],
      "source": [
        "!gcrane"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push Open Web UI Image to Artifact Registry\n",
        "\n",
        "This cell pushes the Open Web UI Docker image to your Artifact Registry repository.\n",
        "\n",
        "*   **docker push:** This command pushes a Docker image to a registry.\n",
        "*   **us-central1-docker.pkg.dev/$PROJECT_ID/ollama-sidecar-codelab-repo/openwebui:** Specifies the destination for the image, which is your Artifact Registry repository in the specified location and project, with the repository name \"ollama-sidecar-codelab-repo\" and the image name \"openwebui\"."
      ],
      "metadata": {
        "id": "zHdDqZOpjPZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0PXSTxmBM6k"
      },
      "outputs": [],
      "source": [
        "!docker push us-central1-docker.pkg.dev/$PROJECT_ID/ollama-sidecar-codelab-repo/openwebui"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Knative Service Configuration\n",
        "\n",
        "This cell defines the configuration for a Knative service using a YAML manifest.\n",
        "\n",
        "1. **Import os:** Imports the `os` module for environment variables.\n",
        "2. **Get Project ID:** Sets the `PROJECT_ID` variable. You can provide a value or it will automatically retrieve the project ID from the Colab environment.\n",
        "3. **Define YAML Content:** Defines the `yaml_content` variable containing the YAML configuration for the Knative service. This configuration specifies:\n",
        "    * **API Version and Kind:** Sets the API version to `serving.knative.dev/v1` and the kind to `Service`, indicating a Knative service.\n",
        "    * **Metadata:** Defines metadata like the service name (`ollama-sidecar-codelab`) and labels.\n",
        "    * **Spec:** Specifies the service's specifications, including:\n",
        "        * **Template:** Defines the template for the service's pods, including annotations for autoscaling, resource limits, and container dependencies.\n",
        "        * **Containers:** Defines two containers:\n",
        "            * `openwebui`: The container for the Open Web UI, using the image from Artifact Registry.\n",
        "            * `ollama-sidecar`: The container for the Ollama model server, also using the image from Artifact Registry.\n",
        "        * **Volumes:** Defines volumes for in-memory storage used by the containers.\n",
        "4. **Write the YAML content to service.yaml** This line creates a yaml file called `service.yaml` and writes all the above yaml content into this file."
      ],
      "metadata": {
        "id": "9xM18zZajr18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jjgVqFSBM6l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get the PROJECT_ID from environment variables\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "yaml_content = f\"\"\"\n",
        "apiVersion: serving.knative.dev/v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: ollama-sidecar-codelab\n",
        "  labels:\n",
        "    cloud.googleapis.com/location: us-central1\n",
        "spec:\n",
        "  template:\n",
        "    metadata:\n",
        "      annotations:\n",
        "        autoscaling.knative.dev/maxScale: '5'\n",
        "        run.googleapis.com/cpu-throttling: 'false'\n",
        "        run.googleapis.com/startup-cpu-boost: 'true'\n",
        "        run.googleapis.com/container-dependencies: '{{\"openwebui\":[\"ollama-sidecar\"]}}'\n",
        "    spec:\n",
        "      containerConcurrency: 80\n",
        "      timeoutSeconds: 300\n",
        "      containers:\n",
        "      - name: openwebui\n",
        "        image: us-central1-docker.pkg.dev/{PROJECT_ID}/ollama-sidecar-codelab-repo/openwebui\n",
        "        ports:\n",
        "        - name: http1\n",
        "          containerPort: 8080\n",
        "        env:\n",
        "        - name: OLLAMA_BASE_URL\n",
        "          value: http://localhost:11434\n",
        "        - name: WEBUI_AUTH\n",
        "          value: 'false'\n",
        "        resources:\n",
        "          limits:\n",
        "            memory: 2Gi\n",
        "            cpu: '2'\n",
        "        volumeMounts:\n",
        "        - name: in-memory-1\n",
        "          mountPath: /app/backend/data\n",
        "        startupProbe:\n",
        "          timeoutSeconds: 240\n",
        "          periodSeconds: 240\n",
        "          failureThreshold: 1\n",
        "          tcpSocket:\n",
        "            port: 8080\n",
        "      - name: ollama-sidecar\n",
        "        image: us-central1-docker.pkg.dev/{PROJECT_ID}/ollama-sidecar-codelab-repo/ollama-gemma-2b\n",
        "        resources:\n",
        "          limits:\n",
        "            cpu: '6'\n",
        "            memory: 20Gi\n",
        "        volumeMounts:\n",
        "        - name: in-memory-2\n",
        "          mountPath: /root/.ollama\n",
        "        startupProbe:\n",
        "          timeoutSeconds: 1\n",
        "          periodSeconds: 10\n",
        "          failureThreshold: 3\n",
        "          tcpSocket:\n",
        "            port: 11434\n",
        "      volumes:\n",
        "      - name: in-memory-2\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "          sizeLimit: 10Gi\n",
        "      - name: in-memory-1\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "          sizeLimit: 1Gi\n",
        "\"\"\"\n",
        "\n",
        "# Write the YAML content to service.yaml\n",
        "with open(\"service.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"Replaced YOUR_PROJECT_ID with {PROJECT_ID} in service.yaml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Knative Service\n",
        "\n",
        "This cell deploys the Knative service defined in the `service.yaml` file.\n",
        "\n",
        "*   **gcloud beta run services replace:** This command uses the `gcloud` command-line tool to deploy or update a Knative service. The `beta` indicates that it's using the beta version of the `run` command group. `replace` specifies that if the services already exists, replace it with the provided configuration, otherwise create it.\n",
        "*   **service.yaml:** This refers to the YAML file containing the service configuration that was created in the previous cell.\n",
        "*   **--project=$PROJECT_ID:** Specifies the Google Cloud project ID where the service will be deployed."
      ],
      "metadata": {
        "id": "nfcMG472j6bG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWkN6KqFBM6l"
      },
      "outputs": [],
      "source": [
        "!gcloud beta run services replace service.yaml --project=$PROJECT_ID"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}