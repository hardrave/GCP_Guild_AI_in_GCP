{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardrave/GCP_Guild_AI_in_GCP/blob/main/cloud_run_with_gemma2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GCP AOssociation Guild - Gemma2 Model Deployment Guide\n",
        "\n",
        "This collaborative notebook was created for the GCP Guild Association to demonstrate the deployment of the Gemma2 2B parameter model to Google Cloud Run. The notebook provides a step-by-step guide covering model preparation, containerization, and deployment to GCP,"
      ],
      "metadata": {
        "id": "cz0Ri4AGd2Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate with Google Cloud\n",
        "\n",
        "This cell authenticates your Google Cloud account using the `gcloud` command-line tool. It updates the application default credentials (ADC) and runs in quiet mode."
      ],
      "metadata": {
        "id": "lKgDJMujZ1wE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4hQ0xWKpLJ"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login --update-adc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup\n",
        "\n",
        "This section sets up the project ID and location for your Google Cloud resources.\n",
        "\n",
        "*   **PROJECT_ID:**  This variable stores your Google Cloud Project ID. You can provide a value if you want to use a specific project, otherwise it will use the environment variable `GOOGLE_CLOUD_REGION`.\n",
        "*   **LOCATION:** This variable stores the location for your Google Cloud resources. It defaults to \"us-central1\" if not specified."
      ],
      "metadata": {
        "id": "PRC9adQEaBzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S8ixbIAyKpLL"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artifact Registry Repository Name\n",
        "\n",
        "This section defines the name of the Artifact Registry repository that will be used to store the Docker image for the Ollama model.\n",
        "\n",
        "*   **AR_REPOSITORY_NAME:** This variable stores the name of the Artifact Registry repository. It is set to \"michal\" in this case."
      ],
      "metadata": {
        "id": "49MPeDePaE7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5U3MbPkDKpLM"
      },
      "outputs": [],
      "source": [
        "AR_REPOSITORY_NAME = \"michal\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Artifact Registry Repository\n",
        "\n",
        "This cell creates an Artifact Registry repository to store the Docker image for the Ollama model.\n",
        "\n",
        "It uses the `gcloud` command-line tool to create a new repository with the following parameters:\n",
        "\n",
        "*   **repository-format:** Specifies the repository format, which is set to `docker` for storing Docker images.\n",
        "*   **location:** Specifies the location of the repository, using the `LOCATION` variable defined earlier.\n",
        "*   **project:** Specifies the Google Cloud project ID, using the `PROJECT_ID` variable defined earlier."
      ],
      "metadata": {
        "id": "p9rb1X1wbLI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8qPl9A3KpLM"
      },
      "outputs": [],
      "source": [
        "!gcloud artifacts repositories create $AR_REPOSITORY_NAME \\\n",
        "      --repository-format=docker \\\n",
        "      --location=$LOCATION \\\n",
        "      --project=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Name\n",
        "\n",
        "This section defines the name of the Ollama model that will be deployed.\n",
        "\n",
        "*   **MODEL_NAME:** This variable stores the name of the Ollama model, which is set to \"gemma2:2b\" in this case. This likely refers to the \"Gemma 2 2b\" model from Ollama."
      ],
      "metadata": {
        "id": "2cjfK_rjbcSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RKsJ8UYvKpLM"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gemma2:2b\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Dockerfile\n",
        "\n",
        "This section defines and creates the Dockerfile used to build the image for the Ollama model deployment.\n",
        "\n",
        "**Dockerfile Content:**\n",
        "\n",
        "The `dockerfile_content` variable stores the content of the Dockerfile. Here's a breakdown of the key instructions:\n",
        "\n",
        "*   **FROM ollama/ollama:** This line specifies the base image for the Dockerfile, which is the official Ollama image.\n",
        "*   **ENV OLLAMA_HOST ...:** These lines set environment variables for the Ollama server, including the host, port, model directory, debug mode, and model keep-alive settings.\n",
        "*   **ENV MODEL {MODEL_NAME}:** This line sets the `MODEL` environment variable to the `MODEL_NAME` defined earlier, specifying the model to load.\n",
        "*   **RUN ollama serve ...:** This line starts the Ollama server, waits for it to start, and then pulls the specified model weights.\n",
        "*   **ENTRYPOINT [\"/bin/sh\"] & CMD [...]:** These lines define the entry point and command to run when the container starts, ensuring the Ollama server starts and loads the model.\n",
        "\n",
        "**Writing the Dockerfile:**\n",
        "\n",
        "The code then writes the `dockerfile_content` to a file named \"Dockerfile\" in the current directory. This Dockerfile will be used in the next step to build the Docker image."
      ],
      "metadata": {
        "id": "ivu6nB6ObpzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Wi-jdKQKpLN"
      },
      "outputs": [],
      "source": [
        "dockerfile_content = f\"\"\"\n",
        "FROM ollama/ollama\n",
        "\n",
        "# Set the host and port to listen on\n",
        "ENV OLLAMA_HOST 0.0.0.0:8080\n",
        "\n",
        "# Set the directory to store model weight files\n",
        "ENV OLLAMA_MODELS /models\n",
        "\n",
        "# Reduce the verbosity of the logs\n",
        "ENV OLLAMA_DEBUG false\n",
        "\n",
        "# Do not unload model weights from the GPU\n",
        "ENV OLLAMA_KEEP_ALIVE -1\n",
        "\n",
        "# Choose the model to load. Ollama defaults to 4-bit quantized weights\n",
        "ENV MODEL {MODEL_NAME}\n",
        "\n",
        "# Start the ollama server and download the model weights\n",
        "RUN ollama serve & sleep 5 && ollama pull $MODEL\n",
        "\n",
        "# At startup time we start the server and run a dummy request\n",
        "# to request the model to be loaded in the GPU memory\n",
        "ENTRYPOINT [\"/bin/sh\"]\n",
        "CMD [\"-c\", \"ollama serve  & (ollama run $MODEL 'Say one word' &) && wait\"]\n",
        "\"\"\"\n",
        "\n",
        "# Write the Dockerfile\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Container URI\n",
        "\n",
        "This section defines the URI for the Docker image that will be built and stored in Artifact Registry.\n",
        "\n",
        "*   **CONTAINER_URI:** This variable stores the full URI of the Docker image in Artifact Registry. It's constructed using the `LOCATION`, `PROJECT_ID`, `AR_REPOSITORY_NAME`, and the image name (\"ollama-gemma-2\"). This URI will be used to push and pull the image."
      ],
      "metadata": {
        "id": "9CDd2Llcbu6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kCdq9uV3KpLN"
      },
      "outputs": [],
      "source": [
        "CONTAINER_URI = (\n",
        "    f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPOSITORY_NAME}/ollama-gemma-2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Push Docker Image\n",
        "\n",
        "This cell builds the Docker image using Cloud Build and pushes it to Artifact Registry.\n",
        "\n",
        "It uses the `gcloud` command-line tool to submit a build request with the following parameters:\n",
        "\n",
        "*   **tag:** Specifies the tag for the Docker image, which is set to the `CONTAINER_URI` defined earlier.\n",
        "*   **project:** Specifies the Google Cloud project ID, using the `PROJECT_ID` variable defined earlier.\n",
        "*   **machine-type:** Specifies the machine type to use for the build, which is set to `e2-highcpu-32` for better performance. This machine type is suitable for CPU-intensive build processes."
      ],
      "metadata": {
        "id": "9d6UK4Oab6ry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9xj-uWbKpLO"
      },
      "outputs": [],
      "source": [
        "!gcloud builds submit --tag $CONTAINER_URI --project $PROJECT_ID --machine-type e2-highcpu-32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloud Run Service Name\n",
        "\n",
        "This section defines the name for the Cloud Run service that will host the Ollama model.\n",
        "\n",
        "*   **SERVICE_NAME:** This variable stores the name of the Cloud Run service. It is set to \"ollama-gemma-2\" in this case. This name will be used to identify and manage the deployed service."
      ],
      "metadata": {
        "id": "lUboHBcRcUkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vXPrcmoZKpLO"
      },
      "outputs": [],
      "source": [
        "SERVICE_NAME = \"ollama-gemma-2\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy to Cloud Run\n",
        "\n",
        "This cell deploys the Ollama model to Cloud Run as a service.\n",
        "\n",
        "It uses the `gcloud` command-line tool with the `beta run deploy` command to create and deploy a new Cloud Run service with the following parameters:\n",
        "\n",
        "*   **SERVICE_NAME:** The name of the Cloud Run service, defined earlier.\n",
        "*   **project:** The Google Cloud project ID.\n",
        "*   **region:** The region where the service will be deployed.\n",
        "*   **image:** The URI of the Docker image in Artifact Registry.\n",
        "*   **concurrency:** The maximum number of requests that can be processed concurrently by a single instance of the service (set to 4).\n",
        "*   **cpu:** The number of CPU cores allocated to each instance (set to 8).\n",
        "*   **max-instances:** The maximum number of instances that can be running for the service (set to 1).\n",
        "*   **memory:** The amount of memory allocated to each instance (set to 16Gi).\n",
        "*   **no-allow-unauthenticated:** Disables unauthenticated access to the service, requiring authentication.\n",
        "*   **no-cpu-throttling:** Disables CPU throttling, ensuring consistent performance.\n",
        "*   **timeout:** The request timeout for the service, in seconds (set to 600)."
      ],
      "metadata": {
        "id": "CVNQVMNDc01q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-6rzL8EKpLO"
      },
      "outputs": [],
      "source": [
        "!gcloud beta run deploy $SERVICE_NAME \\\n",
        "    --project $PROJECT_ID \\\n",
        "    --region $LOCATION \\\n",
        "    --image $CONTAINER_URI \\\n",
        "    --concurrency 4 \\\n",
        "    --cpu 8 \\\n",
        "    --max-instances 1 \\\n",
        "    --memory 16Gi \\\n",
        "    --no-allow-unauthenticated \\\n",
        "    --no-cpu-throttling \\\n",
        "    --timeout=600"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accessing the Deployed Model\n",
        "\n",
        "In order to connect to the deployed Gemma2 model on Google Cloud Run, follow these steps:\n",
        "\n",
        "Start the Cloud Run Proxy:\n",
        "Open a cloud shell terminal and run the following command to start the proxy. When prompted to install the cloud-run-proxy component, choose Y to proceed.\n",
        "```\n",
        "gcloud run services proxy ollama-gemma --port=9090\n",
        "```\n",
        "This will expose the service on localhost:9090.\n",
        "\n",
        "Send a Request to the Model:\n",
        "In a separate terminal tab, while keeping the proxy running, execute the following curl command to send a test request to the model:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# curl http://localhost:9090/api/generate -d '{\n",
        "  \"model\": \"gemma2:2b\",\n",
        "  \"prompt\": \"Why is the sky blue?\"\n",
        "}'\n",
        "```\n",
        "\n",
        "\n",
        "The response will contain the model-generated output based on the provided prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "BJOovtnDZgG8"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}